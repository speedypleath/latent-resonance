{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StyleGAN2-ADA Training — Latent Resonance Spectrograms (Kaggle)\n",
    "\n",
    "Train StyleGAN2-ADA on 512×512 grayscale spectrogram images.\n",
    "ADA (Adaptive Discriminator Augmentation) is purpose-built for limited-data regimes,\n",
    "making it a better fit than StyleGAN3 for small datasets (435–489 images).\n",
    "\n",
    "**Setup:** In the Kaggle sidebar, go to **Settings → Accelerator → GPU T4 x2**.\n",
    "\n",
    "**Dataset:** Upload your `spectrograms.zip` as a [Kaggle Dataset](https://www.kaggle.com/datasets),\n",
    "then add it to this notebook via **Add data** in the sidebar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def run_command(cmd, capture=True, check=False):\n",
    "    \"\"\"Helper function to run shell commands and return output\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, capture_output=capture, text=True, check=check)\n",
    "        if capture:\n",
    "            return result.stdout.strip() if result.stdout else result.stderr.strip()\n",
    "        return result.returncode == 0\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PYTHON ENVIRONMENT SWITCHER FOR KAGGLE V3\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Step 1: Document current environment\n",
    "print(\"[1] DOCUMENTING CURRENT ENVIRONMENT\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Current Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python prefix: {sys.prefix}\")\n",
    "print()\n",
    "\n",
    "print(\"Current conda environments:\")\n",
    "print(run_command(\"conda env list\"))\n",
    "print()\n",
    "\n",
    "print(\"Current Python symlinks:\")\n",
    "print(run_command(\"ls -la /opt/conda/bin/python*\"))\n",
    "print()\n",
    "\n",
    "# Save the original Python executable path for recovery\n",
    "original_python = run_command(\"readlink -f /opt/conda/bin/python3.7\")\n",
    "print(f\"Original Python executable: {original_python}\")\n",
    "print()\n",
    "\n",
    "# Save original Jupyter location\n",
    "original_jupyter = run_command(\"which jupyter\")\n",
    "print(f\"Original Jupyter location: {original_jupyter}\")\n",
    "print()\n",
    "\n",
    "print(\"Backing up symlink information...\")\n",
    "backup_content = run_command(\"ls -la /opt/conda/bin/python*\")\n",
    "with open(\"/tmp/python_symlinks_backup.txt\", \"w\") as f:\n",
    "    f.write(backup_content)\n",
    "    f.write(f\"\\nOriginal Python: {original_python}\")\n",
    "    f.write(f\"\\nOriginal Jupyter: {original_jupyter}\")\n",
    "print(\"Backup saved to /tmp/python_symlinks_backup.txt\")\n",
    "print()\n",
    "\n",
    "# Step 2: Create test scripts\n",
    "print(\"[2] CREATING TEST SCRIPTS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "test_version_code = '''import sys\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Executable: {sys.executable}\")\n",
    "print(f\"Path prefix: {sys.prefix}\")\n",
    "'''\n",
    "\n",
    "with open(\"/tmp/test_version.py\", \"w\") as f:\n",
    "    f.write(test_version_code)\n",
    "print(\"Test scripts created\")\n",
    "print()\n",
    "\n",
    "# Step 3: Test current environment\n",
    "print(\"[3] TESTING ORIGINAL ENVIRONMENT\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Running test with current Python:\")\n",
    "print(run_command(\"python /tmp/test_version.py\"))\n",
    "print()\n",
    "\n",
    "# Step 4: Create new conda environment with Python\n",
    "print(\"[4] CREATING NEW CONDA ENVIRONMENT WITH PYTHON\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create environment with Python 3.7 (or whatever version you prefer)\n",
    "print(\"Creating environment 'newCondaEnvironment' with Python 3.7...\")\n",
    "create_env_cmd = \"conda create -n newCondaEnvironment python=3.7 -c conda-forge -y\"\n",
    "result = run_command(create_env_cmd)\n",
    "print(\"Environment creation completed\")\n",
    "print()\n",
    "\n",
    "# Step 5: Install critical packages in new environment\n",
    "print(\"[5] INSTALLING CRITICAL PACKAGES IN NEW ENVIRONMENT\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Installing Jupyter and other essential packages to prevent kernel failures...\")\n",
    "\n",
    "essential_packages = [\n",
    "    \"jupyter\",\n",
    "    \"jupyter_core\", \n",
    "    \"jupyter_client\",\n",
    "    \"ipykernel\",\n",
    "    \"nbconvert\",\n",
    "    \"papermill\",\n",
    "    \"numpy\",\n",
    "    \"pandas\"\n",
    "]\n",
    "\n",
    "install_cmd = f\"conda install -n newCondaEnvironment {' '.join(essential_packages)} -c conda-forge -y\"\n",
    "print(f\"Installing: {', '.join(essential_packages)}\")\n",
    "result = run_command(install_cmd)\n",
    "print(\"Essential packages installation completed\")\n",
    "print()\n",
    "\n",
    "# If cctbx channel is needed\n",
    "if False:  # Set to True if you need cctbx channel\n",
    "    print(\"Adding cctbx202208 channel packages...\")\n",
    "    add_channel_cmd = \"conda install -n newCondaEnvironment -c cctbx202208 -c conda-forge --override-channels python -y\"\n",
    "    result = run_command(add_channel_cmd)\n",
    "    print(\"Channel packages added\")\n",
    "    print()\n",
    "\n",
    "# Step 6: Verify new environment installation\n",
    "print(\"[6] VERIFYING NEW ENVIRONMENT\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "env_path = \"/opt/conda/envs/newCondaEnvironment\"\n",
    "print(f\"Checking if environment exists at {env_path}...\")\n",
    "if os.path.exists(env_path):\n",
    "    print(\"✓ Environment directory exists\")\n",
    "else:\n",
    "    print(\"✗ Environment directory not found!\")\n",
    "print()\n",
    "\n",
    "# Look for Python in the new environment\n",
    "print(\"Looking for Python executables in new environment:\")\n",
    "python_locations = [\n",
    "    f\"{env_path}/bin/python\",\n",
    "    f\"{env_path}/bin/python3\",\n",
    "    f\"{env_path}/bin/python3.7\"\n",
    "]\n",
    "\n",
    "new_python_path = None\n",
    "for path in python_locations:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"✓ Found: {path}\")\n",
    "        if new_python_path is None:\n",
    "            new_python_path = path\n",
    "            version = run_command(f\"{path} --version\")\n",
    "            print(f\"  Version: {version}\")\n",
    "    else:\n",
    "        print(f\"✗ Not found: {path}\")\n",
    "print()\n",
    "\n",
    "# Check for Jupyter in new environment\n",
    "new_jupyter_path = f\"{env_path}/bin/jupyter\"\n",
    "if os.path.exists(new_jupyter_path):\n",
    "    print(f\"✓ Jupyter found in new environment: {new_jupyter_path}\")\n",
    "else:\n",
    "    print(f\"✗ Jupyter NOT found in new environment!\")\n",
    "print()\n",
    "\n",
    "if new_python_path and os.path.exists(new_python_path):\n",
    "    print(f\"Using Python at: {new_python_path}\")\n",
    "    print(f\"Version: {run_command(f'{new_python_path} --version')}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 7: Remove old symlinks (but keep jupyter working)\n",
    "    print(\"[7] REMOVING OLD PYTHON SYMLINKS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Removing existing Python symlinks...\")\n",
    "    \n",
    "    # Only remove Python symlinks, not jupyter or other tools\n",
    "    symlinks_to_remove = [\n",
    "        \"/opt/conda/bin/python\",\n",
    "        \"/opt/conda/bin/python3\"\n",
    "    ]\n",
    "    \n",
    "    for symlink in symlinks_to_remove:\n",
    "        if os.path.islink(symlink):\n",
    "            result = run_command(f\"sudo rm -f {symlink}\")\n",
    "            print(f\"  Removed: {symlink}\")\n",
    "    \n",
    "    print(\"Old Python symlinks removed\")\n",
    "    print()\n",
    "    \n",
    "    # Step 8: Create new symlinks\n",
    "    print(\"[8] CREATING NEW SYMLINKS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Creating new symlinks to {new_python_path}...\")\n",
    "    \n",
    "    symlinks_to_create = {\n",
    "        \"/opt/conda/bin/python\": new_python_path,\n",
    "        \"/opt/conda/bin/python3\": new_python_path\n",
    "    }\n",
    "    \n",
    "    for symlink, target in symlinks_to_create.items():\n",
    "        result = run_command(f\"sudo ln -sf {target} {symlink}\")\n",
    "        print(f\"  Created: {symlink} -> {target}\")\n",
    "    \n",
    "    # Also update Jupyter symlink if new one exists\n",
    "    if os.path.exists(new_jupyter_path):\n",
    "        print(f\"Updating Jupyter symlink...\")\n",
    "        run_command(f\"sudo rm -f /opt/conda/bin/jupyter\")\n",
    "        run_command(f\"sudo ln -sf {new_jupyter_path} /opt/conda/bin/jupyter\")\n",
    "        print(f\"  Created: /opt/conda/bin/jupyter -> {new_jupyter_path}\")\n",
    "    \n",
    "    print(\"New symlinks created\")\n",
    "    print()\n",
    "    \n",
    "    # Step 9: Verify symlink changes\n",
    "    print(\"[9] VERIFYING SYMLINK CHANGES\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Python symlinks:\")\n",
    "    print(run_command(\"ls -la /opt/conda/bin/python /opt/conda/bin/python3\"))\n",
    "    print()\n",
    "    \n",
    "    print(\"Jupyter symlink:\")\n",
    "    print(run_command(\"ls -la /opt/conda/bin/jupyter\"))\n",
    "    print()\n",
    "    \n",
    "    print(\"Verifying targets:\")\n",
    "    for symlink in [\"/opt/conda/bin/python\", \"/opt/conda/bin/python3\", \"/opt/conda/bin/jupyter\"]:\n",
    "        if os.path.exists(symlink):\n",
    "            target = run_command(f\"readlink -f {symlink}\")\n",
    "            print(f\"  {symlink} -> {target}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 10: Test new environment\n",
    "    print(\"[10] TESTING NEW ENVIRONMENT\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Python version after switch:\")\n",
    "    print(f\"  python: {run_command('python --version')}\")\n",
    "    print(f\"  python3: {run_command('python3 --version')}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Jupyter version:\")\n",
    "    jupyter_version = run_command(\"jupyter --version\")\n",
    "    print(jupyter_version)\n",
    "    print()\n",
    "    \n",
    "    print(\"Testing Jupyter modules:\")\n",
    "    test_jupyter_cmd = 'python -c \"import jupyter_core; print(f\\'jupyter_core: {jupyter_core.__version__}\\')\"'\n",
    "    test_jupyter = run_command(test_jupyter_cmd)\n",
    "    print(test_jupyter)\n",
    "    print()\n",
    "    \n",
    "    print(\"Running test script:\")\n",
    "    print(run_command(\"python /tmp/test_version.py\"))\n",
    "    print()\n",
    "    \n",
    "    # Step 11: Test Python imports\n",
    "    print(\"[11] TESTING PYTHON IMPORTS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Test basic imports\n",
    "    print(\"Testing basic imports:\")\n",
    "    basic_test_cmd = 'python -c \"import sys, os; print(f\\'✓ Basic imports OK. Python {sys.version.split()[0]} at {sys.executable}\\')\"'\n",
    "    basic_test = run_command(basic_test_cmd)\n",
    "    print(basic_test)\n",
    "    print()\n",
    "    \n",
    "    # Test packages with proper syntax\n",
    "    packages_to_test = [\"numpy\", \"pandas\", \"matplotlib\", \"sklearn\", \"jupyter_core\", \"ipykernel\"]\n",
    "    print(\"Testing package availability:\")\n",
    "    for pkg in packages_to_test:\n",
    "        # Use simpler syntax that works in all Python versions\n",
    "        test_cmd = f\"\"\"python -c \"\n",
    "try:\n",
    "    import {pkg}\n",
    "    print('  ✓ {pkg}: ' + str(getattr({pkg}, '__version__', 'version unknown')))\n",
    "except ImportError:\n",
    "    print('  ✗ {pkg}: Not installed')\n",
    "\" \"\"\"\n",
    "        result = run_command(test_cmd)\n",
    "        if result:\n",
    "            print(result)\n",
    "    print()\n",
    "    \n",
    "    # Step 12: Create rollback script\n",
    "    print(\"[12] CREATING ROLLBACK CAPABILITY\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    rollback_code = f'''\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"Rolling back Python environment changes...\")\n",
    "\n",
    "# Remove modified symlinks\n",
    "symlinks = [\"/opt/conda/bin/python\", \"/opt/conda/bin/python3\", \"/opt/conda/bin/jupyter\"]\n",
    "\n",
    "for symlink in symlinks:\n",
    "    if os.path.islink(symlink):\n",
    "        subprocess.run(f\"sudo rm -f {{symlink}}\", shell=True)\n",
    "        print(f\"Removed: {{symlink}}\")\n",
    "\n",
    "# Restore original symlinks\n",
    "original_python = \"{original_python}\"\n",
    "original_jupyter = \"{original_jupyter}\"\n",
    "\n",
    "if os.path.exists(original_python):\n",
    "    subprocess.run(f\"sudo ln -sf {{original_python}} /opt/conda/bin/python\", shell=True)\n",
    "    subprocess.run(f\"sudo ln -sf {{original_python}} /opt/conda/bin/python3\", shell=True)\n",
    "    print(f\"Restored Python symlinks to: {{original_python}}\")\n",
    "\n",
    "if os.path.exists(original_jupyter):\n",
    "    subprocess.run(f\"sudo ln -sf {{original_jupyter}} /opt/conda/bin/jupyter\", shell=True)\n",
    "    print(f\"Restored Jupyter symlink to: {{original_jupyter}}\")\n",
    "else:\n",
    "    # Fallback: reinstall\n",
    "    print(\"Reinstalling Python 3.7 and Jupyter...\")\n",
    "    subprocess.run(\"conda install -n base python=3.7 jupyter -y\", shell=True)\n",
    "\n",
    "# Verify rollback\n",
    "result = subprocess.run(\"python --version\", shell=True, capture_output=True, text=True)\n",
    "print(f\"Python version after rollback: {{result.stdout.strip()}}\")\n",
    "result = subprocess.run(\"jupyter --version\", shell=True, capture_output=True, text=True)\n",
    "print(f\"Jupyter after rollback: {{result.stdout.strip()[:50]}}\")\n",
    "print(\"Rollback complete!\")\n",
    "'''\n",
    "    \n",
    "    with open(\"/tmp/rollback_python.py\", \"w\") as f:\n",
    "        f.write(rollback_code)\n",
    "    \n",
    "    print(\"Rollback script created at /tmp/rollback_python.py\")\n",
    "    print(\"To rollback, run: exec(open('/tmp/rollback_python.py').read())\")\n",
    "    print()\n",
    "    \n",
    "    # Step 13: Final summary\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ENVIRONMENT SWITCH COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"SUMMARY:\")\n",
    "    print(f\"  ✓ Original environment backed up\")\n",
    "    print(f\"  ✓ New conda environment created with Python 3.9\")\n",
    "    print(f\"  ✓ Jupyter and essential packages installed\")\n",
    "    print(f\"  ✓ Python symlinks updated\")\n",
    "    print(f\"  ✓ Jupyter functionality preserved\")\n",
    "    print()\n",
    "    print(\"CURRENT STATE:\")\n",
    "    current_version = run_command('python --version')\n",
    "    current_location = run_command('which python')\n",
    "    print(f\"  Python version: {current_version}\")\n",
    "    print(f\"  Python location: {current_location}\")\n",
    "    \n",
    "    # Check Jupyter status without f-string issues\n",
    "    jupyter_check_cmd = 'jupyter --version'\n",
    "    jupyter_check = run_command(jupyter_check_cmd)\n",
    "    jupyter_status = '✓ Working' if 'jupyter' in jupyter_check else '✗ Not working'\n",
    "    print(f\"  Jupyter status: {jupyter_status}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"IMPORTANT NOTES:\")\n",
    "    print(\"  1. The Jupyter kernel itself is still running the original Python\")\n",
    "    print(\"  2. Only subprocess calls (!command) will use the new Python\")\n",
    "    print(\"  3. Jupyter should continue working normally\")\n",
    "    print(\"  4. To rollback: run exec(open('/tmp/rollback_python.py').read())\")\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Final test - fixed f-string issue\n",
    "    print(\"FINAL TEST - Verify everything works:\")\n",
    "    print(f\"Python: {run_command('python --version')}\")\n",
    "    \n",
    "    # Test jupyter_core import without f-string backslash issue\n",
    "    import_test_cmd = 'python -c \"import jupyter_core; print(True)\"'\n",
    "    import_test_result = run_command(import_test_cmd)\n",
    "    print(f\"Can import jupyter_core: {import_test_result}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ERROR: Could not find or install a new Python version!\")\n",
    "    print(\"The environment switch failed.\")\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb  2 12:59:00 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   49C    P8             10W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   44C    P8             14W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "/bin/bash: line 1: conda: command not found\n",
      "/bin/bash: line 1: /opt/conda/bin/activate: No such file or directory\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (8.3.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: pyspng in /usr/local/lib/python3.12/dist-packages (0.1.4)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
      "Requirement already satisfied: imageio-ffmpeg==0.4.3 in /usr/local/lib/python3.12/dist-packages (0.4.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pyspng) (2.0.2)\n",
      "PyTorch 2.8.0+cu126, CUDA 12.6, GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!conda create -n py37 python=3.7 anaconda --yes\n",
    "!source /opt/conda/bin/activate py37 && conda install -c py37 python -y\n",
    "\n",
    "!pip install -q ninja\n",
    "!pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3\n",
    "\n",
    "import torch\n",
    "assert torch.cuda.is_available(), \"No GPU — enable it in Settings → Accelerator → GPU T4 x2\"\n",
    "print(f\"PyTorch {torch.__version__}, CUDA {torch.version.cuda}, GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone StyleGAN2-ADA & Apply Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "# Fresh clone\n",
    "if os.path.exists(\"stylegan2-ada-pytorch\"):\n",
    "    shutil.rmtree(\"stylegan2-ada-pytorch\")\n",
    "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
    "sys.path.insert(0, \"stylegan2-ada-pytorch\")\n",
    "\n",
    "# ── Patch 1: Fix InfiniteSampler for PyTorch >=2.4 ────────────────────────\n",
    "misc_path = pathlib.Path(\"stylegan2-ada-pytorch/torch_utils/misc.py\")\n",
    "src = misc_path.read_text()\n",
    "src = src.replace(\"super().__init__(dataset)\", \"super().__init__()\")\n",
    "\n",
    "# ── Patch 2: Skip shape-mismatched params during resume ──────────────────\n",
    "#    (needed for RGB ffhq512 -> 1-ch grayscale spectrograms)\n",
    "#    Line-by-line patch to avoid fragile exact-string matching.\n",
    "lines = src.split('\\n')\n",
    "new_lines = []\n",
    "patched = False\n",
    "for line in lines:\n",
    "    if 'tensor.copy_(src_tensors[name].detach())' in line and not patched:\n",
    "        indent = line[:len(line) - len(line.lstrip())]\n",
    "        new_lines.append(f'{indent}if src_tensors[name].shape != tensor.shape:')\n",
    "        new_lines.append(f'{indent}    continue')\n",
    "        new_lines.append(line)\n",
    "        patched = True\n",
    "    else:\n",
    "        new_lines.append(line)\n",
    "src = '\\n'.join(new_lines)\n",
    "assert patched, \"ERROR: could not patch copy_params_and_buffers\"\n",
    "print(f\"  Patch 2: shape-mismatch guard {'applied' if patched else 'FAILED'}\")\n",
    "\n",
    "misc_path.write_text(src)\n",
    "print(f\"Patched {misc_path}\")\n",
    "\n",
    "# ── Patch 3: Fix Adam betas int -> float for PyTorch >=2.9 ────────────────\n",
    "train_path = pathlib.Path(\"stylegan2-ada-pytorch/train.py\")\n",
    "src = train_path.read_text()\n",
    "src = src.replace(\"betas=[0,0.99]\", \"betas=[0.0,0.99]\")\n",
    "train_path.write_text(src)\n",
    "print(f\"Patched {train_path}: Adam betas fix\")\n",
    "\n",
    "# ── Patch 4: Try CUDA ops compilation ────────────────────────────────────\n",
    "cc_major, cc_minor = torch.cuda.get_device_capability(0)\n",
    "arch = f\"{cc_major}.{cc_minor}\"\n",
    "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = arch\n",
    "os.environ[\"TORCH_EXTENSIONS_DIR\"] = \"/tmp/torch_extensions\"\n",
    "if os.path.exists(\"/tmp/torch_extensions\"):\n",
    "    shutil.rmtree(\"/tmp/torch_extensions\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"python\", \"-c\",\n",
    "     \"import sys; sys.path.insert(0,'stylegan2-ada-pytorch'); \"\n",
    "     \"from torch_utils.ops import bias_act; \"\n",
    "     \"assert bias_act._init(), 'init failed'\"],\n",
    "    capture_output=True, text=True, timeout=180,\n",
    ")\n",
    "\n",
    "CUDA_OPS_OK = result.returncode == 0\n",
    "if CUDA_OPS_OK:\n",
    "    print(f\"Custom CUDA ops compiled for sm_{cc_major}{cc_minor} — using fused kernels\")\n",
    "else:\n",
    "    print(f\"CUDA ops compilation failed (arch {arch}), using native PyTorch fallback\")\n",
    "    print(f\"  Error: ...{result.stderr[-300:]}\")\n",
    "    ops_dir = pathlib.Path(\"stylegan2-ada-pytorch/torch_utils/ops\")\n",
    "    for name in [\"bias_act.py\", \"upfirdn2d.py\"]:\n",
    "        p = ops_dir / name\n",
    "        s = p.read_text()\n",
    "        s = s.replace(\"def _init():\", \"def _init():\\n    return False\")\n",
    "        p.write_text(s)\n",
    "        print(f\"  Patched {p}\")\n",
    "\n",
    "# ── Verify patch applied correctly ───────────────────────────────────────\n",
    "verify = misc_path.read_text()\n",
    "assert \"if src_tensors[name].shape != tensor.shape:\" in verify, \\\n",
    "    \"FATAL: shape-mismatch patch missing from misc.py after write!\"\n",
    "print(\"Verified: shape-mismatch guard present in misc.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset\n",
    "\n",
    "Kaggle datasets are mounted at `/kaggle/input/<dataset-name>/`.\n",
    "\n",
    "Set `KAGGLE_DATASET` to match your dataset name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "KAGGLE_DATASET = \"spectrograms\"  # <-- your Kaggle dataset name\n",
    "\n",
    "input_dir = f\"/kaggle/input/{KAGGLE_DATASET}\"\n",
    "\n",
    "# Find PNGs (may be in root or a subfolder)\n",
    "pngs = glob.glob(f\"{input_dir}/**/*.png\", recursive=True)\n",
    "\n",
    "# Copy to a writable working directory (Kaggle input is read-only)\n",
    "DATASET_PATH = \"/kaggle/working/spectrograms\"\n",
    "os.makedirs(DATASET_PATH, exist_ok=True)\n",
    "for p in pngs:\n",
    "    shutil.copy(p, DATASET_PATH)\n",
    "\n",
    "print(f\"Found {len(pngs)} PNG files → copied to {DATASET_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python stylegan2-ada-pytorch/dataset_tool.py \\\n",
    "    --source={DATASET_PATH} \\\n",
    "    --dest=./spectrograms.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "GPUS = 1              # single GPU — avoids multi-GPU sync overhead on T4\n",
    "GAMMA = 3.0           # higher R1 regularization for small datasets\n",
    "SNAP = 5              # snapshot every 5 ticks\n",
    "KIMG = 500            # fits in one Kaggle session\n",
    "AUG = \"ada\"           # adaptive discriminator augmentation\n",
    "TARGET = 0.6          # ADA target heuristic — good default for small datasets\n",
    "MIRROR = False        # no mirroring — horizontal flip would reverse time axis\n",
    "METRICS = \"none\"\n",
    "BATCH_SIZE = 4        # lower VRAM pressure — 12.4 GB at batch=8 was too tight\n",
    "RESUME = \"ffhq512\"    # patch in cell 5 handles RGB->grayscale shape mismatch\n",
    "\n",
    "print(f\"Config: gpus={GPUS}, batch={BATCH_SIZE}, gamma={GAMMA}, aug={AUG}, target={TARGET}, mirror={MIRROR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "resume_flag = f\"--resume={RESUME}\" if RESUME else \"\"\n",
    "mirror_int = 1 if MIRROR else 0\n",
    "\n",
    "!python stylegan2-ada-pytorch/train.py \\\n",
    "    --outdir=./training-runs \\\n",
    "    --cfg=auto \\\n",
    "    --data=./spectrograms.zip \\\n",
    "    --gpus={GPUS} \\\n",
    "    --batch={BATCH_SIZE} \\\n",
    "    --gamma={GAMMA} \\\n",
    "    --snap={SNAP} \\\n",
    "    --kimg={KIMG} \\\n",
    "    --aug={AUG} \\\n",
    "    --target={TARGET} \\\n",
    "    --mirror={mirror_int} \\\n",
    "    --metrics={METRICS} \\\n",
    "    {resume_flag}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reconstruct Audio from Generated Spectrograms\n",
    "\n",
    "Use Griffin-Lim phase estimation to convert the generated spectrogram images back into audio waveforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "pkls = sorted(glob.glob(\"training-runs/**/*.pkl\", recursive=True))\n",
    "assert pkls, \"No snapshots found — has training completed at least one snapshot?\"\n",
    "latest_pkl = pkls[-1]\n",
    "print(f\"Loading {latest_pkl}\")\n",
    "\n",
    "with open(latest_pkl, \"rb\") as f:\n",
    "    G = pickle.load(f)[\"G_ema\"].cuda().eval()\n",
    "\n",
    "NUM_SAMPLES = 5\n",
    "z = torch.randn(NUM_SAMPLES, G.z_dim, device=\"cuda\")\n",
    "with torch.no_grad():\n",
    "    imgs = G(z, None)\n",
    "\n",
    "fig, axes = plt.subplots(1, NUM_SAMPLES, figsize=(20, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    img = imgs[i, 0].cpu().numpy()\n",
    "    ax.imshow(img, cmap=\"magma\", aspect=\"auto\")\n",
    "    ax.set_title(f\"Sample {i}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Generated Spectrograms (StyleGAN2-ADA)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results\n",
    "\n",
    "Kaggle persists everything in `/kaggle/working/` as notebook output.\n",
    "Click **Save Version** (top right) → the training-runs zip will be available under **Output**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q librosa soundfile\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "\n",
    "SR = 22050\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "N_MELS = 512\n",
    "N_ITER = 32\n",
    "DB_RANGE = 80.0\n",
    "\n",
    "output_dir = \"/kaggle/working/reconstructed_audio\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i in range(NUM_SAMPLES):\n",
    "    # Extract spectrogram as numpy array in [-1, 1]\n",
    "    spec = imgs[i, 0].cpu().numpy()\n",
    "\n",
    "    # [-1, 1] → dB → power → linear STFT → Griffin-Lim\n",
    "    S_db = (spec + 1.0) * (DB_RANGE / 2.0) - DB_RANGE\n",
    "    S_power = librosa.db_to_power(S_db, ref=1.0)\n",
    "    S_stft = librosa.feature.inverse.mel_to_stft(S_power, sr=SR, n_fft=N_FFT, power=2.0)\n",
    "    audio = librosa.griffinlim(S_stft, n_iter=N_ITER, hop_length=HOP_LENGTH, n_fft=N_FFT)\n",
    "\n",
    "    # Normalise to -1 dBFS peak\n",
    "    peak = np.abs(audio).max()\n",
    "    if peak > 0:\n",
    "        audio = audio / peak * 10 ** (-1.0 / 20.0)\n",
    "\n",
    "    # Save WAV\n",
    "    wav_path = f\"{output_dir}/sample_{i}.wav\"\n",
    "    sf.write(wav_path, audio, SR)\n",
    "    print(f\"Sample {i}: {len(audio)} samples ({len(audio)/SR:.2f}s) → {wav_path}\")\n",
    "\n",
    "    # Inline audio player\n",
    "    ipd.display(ipd.Audio(audio, rate=SR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(\"/kaggle/working/training-runs\", \"zip\", \".\", \"training-runs\")\n",
    "shutil.make_archive(\"/kaggle/working/reconstructed_audio\", \"zip\", \".\", \"reconstructed_audio\")\n",
    "print(\"Created /kaggle/working/training-runs.zip\")\n",
    "print(\"Created /kaggle/working/reconstructed_audio.zip\")\n",
    "print(\"These will be saved as notebook output when you click Save Version.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
