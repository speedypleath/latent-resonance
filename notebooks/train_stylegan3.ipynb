{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StyleGAN3 Training â€” Latent Resonance Spectrograms\n",
    "\n",
    "Train StyleGAN3 on 512x512 grayscale spectrogram images for the **Latent Resonance: Facial Acoustics** project.\n",
    "\n",
    "Uses the **stylegan3-t** (translation-equivariant) configuration, which is well-suited for spectrograms where the horizontal axis represents time.\n",
    "\n",
    "**Requirements:** Run this notebook on Google Colab with a GPU runtime (T4 or better)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb  1 22:16:24 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   47C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Collecting ninja\n",
      "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n",
      "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ninja\n",
      "Successfully installed ninja-1.13.0\n",
      "PyTorch 2.9.0+cu126, CUDA 12.6, GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "# Reinstall PyTorch with CUDA support (pip install torch can pull CPU-only builds)\n",
    "!pip install torch torchvision\n",
    "!pip install ninja pillow requests\n",
    "\n",
    "import torch\n",
    "assert torch.cuda.is_available(), \"CUDA not available â€” make sure you selected a GPU runtime (Runtime â†’ Change runtime type â†’ GPU)\"\n",
    "print(f\"PyTorch {torch.__version__}, CUDA {torch.version.cuda}, GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone StyleGAN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'stylegan3'...\n",
      "remote: Enumerating objects: 212, done.\u001b[K\n",
      "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
      "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
      "remote: Total 212 (delta 99), reused 90 (delta 90), pack-reused 49 (from 1)\u001b[K\n",
      "Receiving objects: 100% (212/212), 4.16 MiB | 9.45 MiB/s, done.\n",
      "Resolving deltas: 100% (108/108), done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "# Remove previous clone so patches apply cleanly\n",
    "if os.path.exists(\"stylegan3\"):\n",
    "    shutil.rmtree(\"stylegan3\")\n",
    "\n",
    "!git clone https://github.com/NVlabs/stylegan3.git\n",
    "\n",
    "sys.path.insert(0, \"stylegan3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched stylegan3/torch_utils/misc.py: InfiniteSampler fix\n",
      "Patched stylegan3/train.py: Adam betas fix\n",
      "CUDA ops compilation failed, falling back to native PyTorch + reduced model\n",
      "  stderr (last 300 chars): ...^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1324, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'bias_act_plugin'\n",
      "\n",
      "  Patched stylegan3/torch_utils/ops/bias_act.py\n",
      "  Patched stylegan3/torch_utils/ops/upfirdn2d.py\n",
      "  Patched stylegan3/torch_utils/ops/filtered_lrelu.py\n",
      "  Will use --cbase=16384 --cmax=256 to fit in T4 VRAM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "# â”€â”€ Patch 1: Fix InfiniteSampler for PyTorch â‰¥2.4 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "misc_path = pathlib.Path(\"stylegan3/torch_utils/misc.py\")\n",
    "src = misc_path.read_text()\n",
    "src = src.replace(\"super().__init__(dataset)\", \"super().__init__()\")\n",
    "misc_path.write_text(src)\n",
    "print(f\"Patched {misc_path}: InfiniteSampler fix\")\n",
    "\n",
    "# â”€â”€ Patch 2: Fix Adam betas int â†’ float for PyTorch â‰¥2.9 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_path = pathlib.Path(\"stylegan3/train.py\")\n",
    "src = train_path.read_text()\n",
    "src = src.replace(\"betas=[0,0.99]\", \"betas=[0.0,0.99]\")\n",
    "train_path.write_text(src)\n",
    "print(f\"Patched {train_path}: Adam betas fix\")\n",
    "\n",
    "# â”€â”€ Patch 3: Try CUDA ops compilation, fall back if it fails â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Set env vars for CUDA compilation (T4 = compute capability 7.5)\n",
    "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"7.5\"\n",
    "os.environ[\"TORCH_EXTENSIONS_DIR\"] = \"/tmp/torch_extensions\"\n",
    "\n",
    "# Clean stale builds\n",
    "if os.path.exists(\"/tmp/torch_extensions\"):\n",
    "    shutil.rmtree(\"/tmp/torch_extensions\")\n",
    "\n",
    "# Test compilation in a subprocess (same env as training will use)\n",
    "result = subprocess.run(\n",
    "    [\"python\", \"-c\",\n",
    "     \"import sys; sys.path.insert(0,'stylegan3'); \"\n",
    "     \"from torch_utils.ops import bias_act; \"\n",
    "     \"assert bias_act._init(), 'init failed'\"],\n",
    "    capture_output=True, text=True, timeout=120,\n",
    ")\n",
    "\n",
    "CUDA_OPS_OK = result.returncode == 0\n",
    "if CUDA_OPS_OK:\n",
    "    print(\"Custom CUDA ops compiled successfully â€” using fused kernels\")\n",
    "else:\n",
    "    print(\"CUDA ops compilation failed, falling back to native PyTorch + reduced model\")\n",
    "    print(f\"  stderr (last 300 chars): ...{result.stderr[-300:]}\")\n",
    "    # Disable custom ops so ref implementations are used\n",
    "    ops_dir = pathlib.Path(\"stylegan3/torch_utils/ops\")\n",
    "    for name in [\"bias_act.py\", \"upfirdn2d.py\", \"filtered_lrelu.py\"]:\n",
    "        p = ops_dir / name\n",
    "        src = p.read_text()\n",
    "        src = src.replace(\"def _init():\", \"def _init():\\n    return False\")\n",
    "        p.write_text(src)\n",
    "        print(f\"  Patched {p}\")\n",
    "    print(\"  Will use --cbase=16384 --cmax=256 to fit in T4 VRAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload Dataset\n",
    "\n",
    "Get your `data/spectrograms/*.png` files into the Colab runtime. Pick **one** of the options below:\n",
    "\n",
    "- **Option A** â€” Download from a shared Google Drive folder/file using `gdown`\n",
    "- **Option B** â€” Download from any public URL using `wget`\n",
    "- **Option C** â€” Drag and drop files into the Colab file browser sidebar (ğŸ“ icon on the left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1x9ABdgP06h00cLygu2qSOXCSCma1sKFA\n",
      "From (redirected): https://drive.google.com/uc?id=1x9ABdgP06h00cLygu2qSOXCSCma1sKFA&confirm=t&uuid=7bfff5b4-b4e6-4d55-9bf2-9f27be8da6ea\n",
      "To: /content/spectrograms.zip\n",
      "100% 57.8M/57.8M [00:00<00:00, 115MB/s] \n",
      "Found 435 PNG files in ./spectrograms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# â”€â”€ Option A: Download a shared Google Drive folder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. Share your Drive folder (Anyone with the link â†’ Viewer)\n",
    "# 2. Copy link â†’ extract the ID after /folders/\n",
    "# Uncomment the two lines below and paste your folder ID:\n",
    "#\n",
    "# !pip install -q gdown\n",
    "# !gdown --folder \"https://drive.google.com/drive/folders/YOUR_FOLDER_ID\" -O ./spectrograms --remaining-ok\n",
    "\n",
    "# â”€â”€ Option B: Download a zip from any URL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Host your spectrograms.zip somewhere (Drive shared link, S3, etc.)\n",
    "# For a Google Drive *file* link, use gdown:\n",
    "#\n",
    "!pip install -q gdown\n",
    "!gdown --fuzzy \"https://drive.google.com/file/d/1x9ABdgP06h00cLygu2qSOXCSCma1sKFA/view?usp=sharing\" -O spectrograms.zip\n",
    "!unzip -qo spectrograms.zip -d ./spectrograms_raw\n",
    "pngs = glob.glob(\"spectrograms_raw/**/*.png\", recursive=True)\n",
    "os.makedirs(\"./spectrograms\", exist_ok=True)\n",
    "for p in pngs:\n",
    "    shutil.copy(p, \"./spectrograms/\")\n",
    "\n",
    "# â”€â”€ Option C: Drag & drop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. Click the ğŸ“ icon in the left sidebar to open the file browser\n",
    "# 2. Create a \"spectrograms\" folder\n",
    "# 3. Drag your PNG files (or a zip) into it\n",
    "# If you uploaded a zip, uncomment:\n",
    "# !unzip -qo spectrograms.zip -d ./spectrograms\n",
    "\n",
    "# â”€â”€ Set the final path (should contain *.png files) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DATASET_PATH = \"./spectrograms\"  # <-- adjust if your images end up elsewhere\n",
    "\n",
    "# Verify\n",
    "pngs = [f for f in os.listdir(DATASET_PATH) if f.endswith(\".png\")]\n",
    "print(f\"Found {len(pngs)} PNG files in {DATASET_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Dataset\n",
    "\n",
    "StyleGAN3 requires images packaged as a zip archive. The `dataset_tool.py` script handles grayscale detection and resolution validation automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure Training\n",
    "\n",
    "Parameters auto-adjust based on whether CUDA ops compiled:\n",
    "\n",
    "| Scenario | `BATCH_SIZE` | `CBASE` | `CMAX` |\n",
    "|----------|-------------|---------|--------|\n",
    "| CUDA ops OK | 8 | 32768 | 512 |\n",
    "| Native fallback | 4 | 16384 | 256 |\n",
    "\n",
    "Reduced `cbase`/`cmax` halve the channel counts, which is needed because the native PyTorch reference ops use significantly more VRAM than the fused CUDA kernels. For a 435-image dataset this capacity is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using reduced model (cbase=8192, cmax=128, batch=2) with native PyTorch ops\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "GPUS = 1\n",
    "GAMMA = 2.0\n",
    "SNAP = 10\n",
    "KIMG = 1000\n",
    "CFG = \"stylegan3-t\"\n",
    "MIRROR = False\n",
    "METRICS = \"none\"\n",
    "RESUME = \"\"\n",
    "\n",
    "# Adjust capacity based on whether CUDA ops compiled\n",
    "if CUDA_OPS_OK:\n",
    "    BATCH_SIZE = 8\n",
    "    CBASE = 32768\n",
    "    CMAX = 512\n",
    "    print(\"Using full model capacity with fused CUDA kernels\")\n",
    "else:\n",
    "    BATCH_SIZE = 2\n",
    "    CBASE = 8192    # quarter â€” ref ops pad/conv are extremely memory-hungry\n",
    "    CMAX = 128\n",
    "    print(f\"Using reduced model (cbase={CBASE}, cmax={CMAX}, batch={BATCH_SIZE}) with native PyTorch ops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training options:\n",
      "{\n",
      "  \"G_kwargs\": {\n",
      "    \"class_name\": \"training.networks_stylegan3.Generator\",\n",
      "    \"z_dim\": 512,\n",
      "    \"w_dim\": 512,\n",
      "    \"mapping_kwargs\": {\n",
      "      \"num_layers\": 2\n",
      "    },\n",
      "    \"channel_base\": 8192,\n",
      "    \"channel_max\": 128,\n",
      "    \"magnitude_ema_beta\": 0.9999306876841536\n",
      "  },\n",
      "  \"D_kwargs\": {\n",
      "    \"class_name\": \"training.networks_stylegan2.Discriminator\",\n",
      "    \"block_kwargs\": {\n",
      "      \"freeze_layers\": 0\n",
      "    },\n",
      "    \"mapping_kwargs\": {},\n",
      "    \"epilogue_kwargs\": {\n",
      "      \"mbstd_group_size\": 2\n",
      "    },\n",
      "    \"channel_base\": 8192,\n",
      "    \"channel_max\": 128\n",
      "  },\n",
      "  \"G_opt_kwargs\": {\n",
      "    \"class_name\": \"torch.optim.Adam\",\n",
      "    \"betas\": [\n",
      "      0.0,\n",
      "      0.99\n",
      "    ],\n",
      "    \"eps\": 1e-08,\n",
      "    \"lr\": 0.0025\n",
      "  },\n",
      "  \"D_opt_kwargs\": {\n",
      "    \"class_name\": \"torch.optim.Adam\",\n",
      "    \"betas\": [\n",
      "      0.0,\n",
      "      0.99\n",
      "    ],\n",
      "    \"eps\": 1e-08,\n",
      "    \"lr\": 0.002\n",
      "  },\n",
      "  \"loss_kwargs\": {\n",
      "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
      "    \"r1_gamma\": 2.0\n",
      "  },\n",
      "  \"data_loader_kwargs\": {\n",
      "    \"pin_memory\": true,\n",
      "    \"prefetch_factor\": 2,\n",
      "    \"num_workers\": 3\n",
      "  },\n",
      "  \"training_set_kwargs\": {\n",
      "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
      "    \"path\": \"./spectrograms.zip\",\n",
      "    \"use_labels\": false,\n",
      "    \"max_size\": 435,\n",
      "    \"xflip\": false,\n",
      "    \"resolution\": 512,\n",
      "    \"random_seed\": 0\n",
      "  },\n",
      "  \"num_gpus\": 1,\n",
      "  \"batch_size\": 2,\n",
      "  \"batch_gpu\": 2,\n",
      "  \"metrics\": [],\n",
      "  \"total_kimg\": 1000,\n",
      "  \"kimg_per_tick\": 4,\n",
      "  \"image_snapshot_ticks\": 10,\n",
      "  \"network_snapshot_ticks\": 10,\n",
      "  \"random_seed\": 0,\n",
      "  \"ema_kimg\": 0.625,\n",
      "  \"augment_kwargs\": {\n",
      "    \"class_name\": \"training.augment.AugmentPipe\",\n",
      "    \"xflip\": 1,\n",
      "    \"rotate90\": 1,\n",
      "    \"xint\": 1,\n",
      "    \"scale\": 1,\n",
      "    \"rotate\": 1,\n",
      "    \"aniso\": 1,\n",
      "    \"xfrac\": 1,\n",
      "    \"brightness\": 1,\n",
      "    \"contrast\": 1,\n",
      "    \"lumaflip\": 1,\n",
      "    \"hue\": 1,\n",
      "    \"saturation\": 1\n",
      "  },\n",
      "  \"ada_target\": 0.6,\n",
      "  \"run_dir\": \"./training-runs/00002-stylegan3-t-spectrograms-gpus1-batch2-gamma2\"\n",
      "}\n",
      "\n",
      "Output directory:    ./training-runs/00002-stylegan3-t-spectrograms-gpus1-batch2-gamma2\n",
      "Number of GPUs:      1\n",
      "Batch size:          2 images\n",
      "Training duration:   1000 kimg\n",
      "Dataset path:        ./spectrograms.zip\n",
      "Dataset size:        435 images\n",
      "Dataset resolution:  512\n",
      "Dataset labels:      False\n",
      "Dataset x-flips:     False\n",
      "\n",
      "Creating output directory...\n",
      "Launching processes...\n",
      "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n",
      "Loading training set...\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "\n",
      "Num images:  435\n",
      "Image shape: [1, 512, 512]\n",
      "Label shape: [0]\n",
      "\n",
      "Constructing networks...\n",
      "\n",
      "Generator                    Parameters  Buffers  Output shape        Datatype\n",
      "---                          ---         ---      ---                 ---     \n",
      "mapping.fc0                  262656      -        [2, 512]            float32 \n",
      "mapping.fc1                  262656      -        [2, 512]            float32 \n",
      "mapping                      -           512      [2, 16, 512]        float32 \n",
      "synthesis.input.affine       2052        -        [2, 4]              float32 \n",
      "synthesis.input              16384       393      [2, 128, 36, 36]    float32 \n",
      "synthesis.L0_36_128.affine   65664       -        [2, 128]            float32 \n",
      "synthesis.L0_36_128          147584      25       [2, 128, 36, 36]    float32 \n",
      "synthesis.L1_36_128.affine   65664       -        [2, 128]            float32 \n",
      "synthesis.L1_36_128          147584      25       [2, 128, 36, 36]    float32 \n",
      "synthesis.L2_52_128.affine   65664       -        [2, 128]            float32 \n",
      "synthesis.L2_52_128          147584      37       [2, 128, 52, 52]    float32 \n",
      "synthesis.L3_52_128.affine   65664       -        [2, 128]            float32 \n",
      "synthesis.L3_52_128          147584      25       [2, 128, 52, 52]    float32 \n",
      "synthesis.L4_84_128.affine   65664       -        [2, 128]            float32 \n",
      "synthesis.L4_84_128          147584      37       [2, 128, 84, 84]    float16 \n",
      "synthesis.L5_84_128.affine   65664       -        [2, 128]            float32 \n",
      "synthesis.L5_84_128          147584      25       [2, 128, 84, 84]    float16 \n",
      "synthesis.L6_148_128.affine  65664       -        [2, 128]            float32 \n",
      "synthesis.L6_148_128         147584      37       [2, 128, 148, 148]  float16 \n",
      "synthesis.L7_148_121.affine  65664       -        [2, 128]            float32 \n",
      "synthesis.L7_148_121         139513      25       [2, 121, 148, 148]  float16 \n",
      "synthesis.L8_276_81.affine   62073       -        [2, 121]            float32 \n",
      "synthesis.L8_276_81          88290       37       [2, 81, 276, 276]   float16 \n",
      "synthesis.L9_276_54.affine   41553       -        [2, 81]             float32 \n",
      "synthesis.L9_276_54          39420       25       [2, 54, 276, 276]   float16 \n",
      "synthesis.L10_532_36.affine  27702       -        [2, 54]             float32 \n",
      "synthesis.L10_532_36         17532       37       [2, 36, 532, 532]   float16 \n",
      "synthesis.L11_532_24.affine  18468       -        [2, 36]             float32 \n",
      "synthesis.L11_532_24         7800        25       [2, 24, 532, 532]   float16 \n",
      "synthesis.L12_532_16.affine  12312       -        [2, 24]             float32 \n",
      "synthesis.L12_532_16         3472        25       [2, 16, 532, 532]   float16 \n",
      "synthesis.L13_512_16.affine  8208        -        [2, 16]             float32 \n",
      "synthesis.L13_512_16         2320        25       [2, 16, 512, 512]   float16 \n",
      "synthesis.L14_512_1.affine   8208        -        [2, 16]             float32 \n",
      "synthesis.L14_512_1          17          1        [2, 1, 512, 512]    float16 \n",
      "synthesis                    -           -        [2, 1, 512, 512]    float32 \n",
      "---                          ---         ---      ---                 ---     \n",
      "Total                        2579036     1316     -                   -       \n",
      "\n",
      "\n",
      "Discriminator  Parameters  Buffers  Output shape       Datatype\n",
      "---            ---         ---      ---                ---     \n",
      "b512.fromrgb   32          16       [2, 16, 512, 512]  float16 \n",
      "b512.skip      512         16       [2, 32, 256, 256]  float16 \n",
      "b512.conv0     2320        16       [2, 16, 512, 512]  float16 \n",
      "b512.conv1     4640        16       [2, 32, 256, 256]  float16 \n",
      "b512           -           16       [2, 32, 256, 256]  float16 \n",
      "b256.skip      2048        16       [2, 64, 128, 128]  float16 \n",
      "b256.conv0     9248        16       [2, 32, 256, 256]  float16 \n",
      "b256.conv1     18496       16       [2, 64, 128, 128]  float16 \n",
      "b256           -           16       [2, 64, 128, 128]  float16 \n",
      "b128.skip      8192        16       [2, 128, 64, 64]   float16 \n",
      "b128.conv0     36928       16       [2, 64, 128, 128]  float16 \n",
      "b128.conv1     73856       16       [2, 128, 64, 64]   float16 \n",
      "b128           -           16       [2, 128, 64, 64]   float16 \n",
      "b64.skip       16384       16       [2, 128, 32, 32]   float16 \n",
      "b64.conv0      147584      16       [2, 128, 64, 64]   float16 \n",
      "b64.conv1      147584      16       [2, 128, 32, 32]   float16 \n",
      "b64            -           16       [2, 128, 32, 32]   float16 \n",
      "b32.skip       16384       16       [2, 128, 16, 16]   float32 \n",
      "b32.conv0      147584      16       [2, 128, 32, 32]   float32 \n",
      "b32.conv1      147584      16       [2, 128, 16, 16]   float32 \n",
      "b32            -           16       [2, 128, 16, 16]   float32 \n",
      "b16.skip       16384       16       [2, 128, 8, 8]     float32 \n",
      "b16.conv0      147584      16       [2, 128, 16, 16]   float32 \n",
      "b16.conv1      147584      16       [2, 128, 8, 8]     float32 \n",
      "b16            -           16       [2, 128, 8, 8]     float32 \n",
      "b8.skip        16384       16       [2, 128, 4, 4]     float32 \n",
      "b8.conv0       147584      16       [2, 128, 8, 8]     float32 \n",
      "b8.conv1       147584      16       [2, 128, 4, 4]     float32 \n",
      "b8             -           16       [2, 128, 4, 4]     float32 \n",
      "b4.mbstd       -           -        [2, 129, 4, 4]     float32 \n",
      "b4.conv        148736      16       [2, 128, 4, 4]     float32 \n",
      "b4.fc          262272      -        [2, 128]           float32 \n",
      "b4.out         129         -        [2, 1]             float32 \n",
      "---            ---         ---      ---                ---     \n",
      "Total          1813617     480      -                  -       \n",
      "\n",
      "Setting up augmentation...\n",
      "Distributing across 1 GPUs...\n",
      "Setting up training phases...\n",
      "Exporting sample images...\n",
      "Initializing logs...\n",
      "2026-02-01 22:43:03.904120: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769985783.935369    7005 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769985783.944247    7005 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769985783.967958    7005 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769985783.967995    7005 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769985783.968003    7005 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769985783.968010    7005 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2026-02-01 22:43:03.974479: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Training for 1000 kimg...\n",
      "\n",
      "tick 0     kimg 0.0      time 1m 34s       sec/tick 57.8    sec/kimg 28914.20 maintenance 36.3   cpumem 2.84   gpumem 8.05   reserved 9.09   augment 0.000\n",
      "tick 1     kimg 4.0      time 41m 48s      sec/tick 2394.6  sec/kimg 598.66  maintenance 19.2   cpumem 2.89   gpumem 5.46   reserved 5.60   augment 0.006\n",
      "tick 2     kimg 8.0      time 1h 23m 38s   sec/tick 2510.0  sec/kimg 627.50  maintenance 0.0    cpumem 2.89   gpumem 5.46   reserved 5.66   augment 0.013\n",
      "tick 3     kimg 12.0     time 2h 06m 03s   sec/tick 2544.8  sec/kimg 636.20  maintenance 0.0    cpumem 2.89   gpumem 5.46   reserved 5.66   augment 0.019\n",
      "tick 4     kimg 16.0     time 2h 49m 44s   sec/tick 2620.8  sec/kimg 655.20  maintenance 0.0    cpumem 2.89   gpumem 5.47   reserved 5.66   augment 0.026\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "resume_flag = f\"--resume={RESUME}\" if RESUME else \"\"\n",
    "mirror_int = 1 if MIRROR else 0\n",
    "\n",
    "!python stylegan3/train.py \\\n",
    "    --outdir=./training-runs \\\n",
    "    --cfg={CFG} \\\n",
    "    --data=./spectrograms.zip \\\n",
    "    --gpus={GPUS} \\\n",
    "    --batch={BATCH_SIZE} \\\n",
    "    --batch-gpu={BATCH_SIZE} \\\n",
    "    --gamma={GAMMA} \\\n",
    "    --snap={SNAP} \\\n",
    "    --kimg={KIMG} \\\n",
    "    --mirror={mirror_int} \\\n",
    "    --metrics={METRICS} \\\n",
    "    --cbase={CBASE} \\\n",
    "    --cmax={CMAX} \\\n",
    "    --mbstd-group={BATCH_SIZE} \\\n",
    "    {resume_flag}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Samples\n",
    "\n",
    "Load the latest checkpoint and generate sample spectrograms to verify training quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Find the latest snapshot pickle\n",
    "pkls = sorted(glob.glob(\"training-runs/**/*.pkl\", recursive=True))\n",
    "assert pkls, \"No snapshots found â€” has training completed at least one snapshot?\"\n",
    "latest_pkl = pkls[-1]\n",
    "print(f\"Loading {latest_pkl}\")\n",
    "\n",
    "with open(latest_pkl, \"rb\") as f:\n",
    "    G = pickle.load(f)[\"G_ema\"].cuda().eval()\n",
    "\n",
    "# Generate 4 samples\n",
    "z = torch.randn(4, G.z_dim, device=\"cuda\")\n",
    "with torch.no_grad():\n",
    "    imgs = G(z, None)\n",
    "\n",
    "# Display as a grid\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    img = imgs[i, 0].cpu().numpy()  # single channel\n",
    "    ax.imshow(img, cmap=\"magma\", aspect=\"auto\")\n",
    "    ax.set_title(f\"Sample {i}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Generated Spectrograms\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results\n",
    "\n",
    "Zip the training results. Download via the Colab file browser (ğŸ“ sidebar â†’ right-click â†’ Download), or upload to Drive with `gdown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Create a zip of the training runs\n",
    "shutil.make_archive(\"training-runs\", \"zip\", \".\", \"training-runs\")\n",
    "print(\"Created training-runs.zip\")\n",
    "print(\"Download it from the file browser: click ğŸ“ in the left sidebar â†’ right-click training-runs.zip â†’ Download\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
