{"cells":[{"cell_type":"markdown","metadata":{},"source":"# StyleGAN2-ADA Training — Latent Resonance Spectrograms (Kaggle)\n\nTrain StyleGAN2-ADA on 512×512 grayscale spectrogram images.\nADA (Adaptive Discriminator Augmentation) is purpose-built for limited-data regimes,\nmaking it a better fit than StyleGAN3 for small datasets (435–489 images).\n\n**Setup:** In the Kaggle sidebar, go to **Settings → Accelerator → GPU T4 x2**.\n\n**Dataset:** Upload your `spectrograms.zip` as a [Kaggle Dataset](https://www.kaggle.com/datasets),\nthen add it to this notebook via **Add data** in the sidebar."},{"cell_type":"markdown","metadata":{},"source":"## 1. Setup & GPU Check"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"!nvidia-smi\n!pip install -q ninja\n\nimport torch\nassert torch.cuda.is_available(), \"No GPU — enable it in Settings → Accelerator → GPU T4 x2\"\nprint(f\"PyTorch {torch.__version__}, CUDA {torch.version.cuda}, GPU: {torch.cuda.get_device_name(0)}\")"},{"cell_type":"markdown","metadata":{},"source":"## 2. Clone StyleGAN2-ADA & Apply Patches"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import os\nimport sys\nimport pathlib\nimport shutil\nimport subprocess\n\n# Fresh clone\nif os.path.exists(\"stylegan2-ada-pytorch\"):\n    shutil.rmtree(\"stylegan2-ada-pytorch\")\n!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\nsys.path.insert(0, \"stylegan2-ada-pytorch\")\n\n# ── Patch 1: Fix InfiniteSampler for PyTorch ≥2.4 ────────────────────────\nmisc_path = pathlib.Path(\"stylegan2-ada-pytorch/torch_utils/misc.py\")\nsrc = misc_path.read_text()\nsrc = src.replace(\"super().__init__(dataset)\", \"super().__init__()\")\nmisc_path.write_text(src)\nprint(f\"Patched {misc_path}: InfiniteSampler fix\")\n\n# ── Patch 2: Fix Adam betas int → float for PyTorch ≥2.9 ────────────────\ntrain_path = pathlib.Path(\"stylegan2-ada-pytorch/train.py\")\nsrc = train_path.read_text()\nsrc = src.replace(\"betas=[0,0.99]\", \"betas=[0.0,0.99]\")\ntrain_path.write_text(src)\nprint(f\"Patched {train_path}: Adam betas fix\")\n\n# ── Patch 3: Try CUDA ops compilation ────────────────────────────────────\ncc_major, cc_minor = torch.cuda.get_device_capability(0)\narch = f\"{cc_major}.{cc_minor}\"\nos.environ[\"TORCH_CUDA_ARCH_LIST\"] = arch\nos.environ[\"TORCH_EXTENSIONS_DIR\"] = \"/tmp/torch_extensions\"\nif os.path.exists(\"/tmp/torch_extensions\"):\n    shutil.rmtree(\"/tmp/torch_extensions\")\n\nresult = subprocess.run(\n    [\"python\", \"-c\",\n     \"import sys; sys.path.insert(0,'stylegan2-ada-pytorch'); \"\n     \"from torch_utils.ops import bias_act; \"\n     \"assert bias_act._init(), 'init failed'\"],\n    capture_output=True, text=True, timeout=180,\n)\n\nCUDA_OPS_OK = result.returncode == 0\nif CUDA_OPS_OK:\n    print(f\"Custom CUDA ops compiled for sm_{cc_major}{cc_minor} — using fused kernels\")\nelse:\n    print(f\"CUDA ops compilation failed (arch {arch}), using native PyTorch fallback\")\n    print(f\"  Error: ...{result.stderr[-300:]}\")\n    ops_dir = pathlib.Path(\"stylegan2-ada-pytorch/torch_utils/ops\")\n    for name in [\"bias_act.py\", \"upfirdn2d.py\"]:\n        p = ops_dir / name\n        s = p.read_text()\n        s = s.replace(\"def _init():\", \"def _init():\\n    return False\")\n        p.write_text(s)\n        print(f\"  Patched {p}\")"},{"cell_type":"markdown","metadata":{},"source":"## 3. Load Dataset\n\nKaggle datasets are mounted at `/kaggle/input/<dataset-name>/`.\n\nSet `KAGGLE_DATASET` to match your dataset name."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import os\nimport glob\nimport shutil\n\nKAGGLE_DATASET = \"spectrograms\"  # <-- your Kaggle dataset name\n\ninput_dir = f\"/kaggle/input/{KAGGLE_DATASET}\"\n\n# Find PNGs (may be in root or a subfolder)\npngs = glob.glob(f\"{input_dir}/**/*.png\", recursive=True)\n\n# Copy to a writable working directory (Kaggle input is read-only)\nDATASET_PATH = \"/kaggle/working/spectrograms\"\nos.makedirs(DATASET_PATH, exist_ok=True)\nfor p in pngs:\n    shutil.copy(p, DATASET_PATH)\n\nprint(f\"Found {len(pngs)} PNG files → copied to {DATASET_PATH}\")"},{"cell_type":"markdown","metadata":{},"source":"## 4. Prepare Dataset"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"!python stylegan2-ada-pytorch/dataset_tool.py \\\n    --source={DATASET_PATH} \\\n    --dest=./spectrograms.zip"},{"cell_type":"markdown","metadata":{},"source":"## 5. Configure Training"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import os\nos.environ[\"PYTORCH_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nGPUS = 1              # single GPU more stable for small datasets\nGAMMA = 10.0          # higher R1 regularization for small datasets\nSNAP = 10\nKIMG = 5000\nAUG = \"ada\"           # adaptive discriminator augmentation\nTARGET = 0.6          # ADA target heuristic — good default for small datasets\nMIRROR = False\nMETRICS = \"none\"\nBATCH_SIZE = 8\nRESUME = \"\"\n\nprint(f\"Config: gpus={GPUS}, batch={BATCH_SIZE}, gamma={GAMMA}, aug={AUG}, target={TARGET}, mirror={MIRROR}\")"},{"cell_type":"markdown","metadata":{},"source":"## 6. Train"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import torch\ntorch.cuda.empty_cache()\n\nresume_flag = f\"--resume={RESUME}\" if RESUME else \"\"\nmirror_int = 1 if MIRROR else 0\n\n!python stylegan2-ada-pytorch/train.py \\\n    --outdir=./training-runs \\\n    --cfg=auto \\\n    --data=./spectrograms.zip \\\n    --gpus={GPUS} \\\n    --batch={BATCH_SIZE} \\\n    --gamma={GAMMA} \\\n    --snap={SNAP} \\\n    --kimg={KIMG} \\\n    --aug={AUG} \\\n    --target={TARGET} \\\n    --mirror={mirror_int} \\\n    --metrics={METRICS} \\\n    {resume_flag}"},{"cell_type":"markdown","metadata":{},"source":"## 7. Generate Samples"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import glob\nimport pickle\n\nimport matplotlib.pyplot as plt\nimport torch\n\npkls = sorted(glob.glob(\"training-runs/**/*.pkl\", recursive=True))\nassert pkls, \"No snapshots found — has training completed at least one snapshot?\"\nlatest_pkl = pkls[-1]\nprint(f\"Loading {latest_pkl}\")\n\nwith open(latest_pkl, \"rb\") as f:\n    G = pickle.load(f)[\"G_ema\"].cuda().eval()\n\nNUM_SAMPLES = 5\nz = torch.randn(NUM_SAMPLES, G.z_dim, device=\"cuda\")\nwith torch.no_grad():\n    imgs = G(z, None)\n\nfig, axes = plt.subplots(1, NUM_SAMPLES, figsize=(20, 4))\nfor i, ax in enumerate(axes):\n    img = imgs[i, 0].cpu().numpy()\n    ax.imshow(img, cmap=\"magma\", aspect=\"auto\")\n    ax.set_title(f\"Sample {i}\")\n    ax.axis(\"off\")\nplt.suptitle(\"Generated Spectrograms (StyleGAN2-ADA)\")\nplt.tight_layout()\nplt.show()"},{"cell_type":"markdown","source":"## 8. Reconstruct Audio from Generated Spectrograms\n\nUse Griffin-Lim phase estimation to convert the generated spectrogram images back into audio waveforms.","metadata":{}},{"cell_type":"code","source":"!pip install -q librosa soundfile\n\nimport numpy as np\nimport librosa\nimport soundfile as sf\nimport IPython.display as ipd\n\nSR = 22050\nN_FFT = 2048\nHOP_LENGTH = 512\nN_MELS = 512\nN_ITER = 32\nDB_RANGE = 80.0\n\noutput_dir = \"/kaggle/working/reconstructed_audio\"\nos.makedirs(output_dir, exist_ok=True)\n\nfor i in range(NUM_SAMPLES):\n    # Extract spectrogram as numpy array in [-1, 1]\n    spec = imgs[i, 0].cpu().numpy()\n\n    # [-1, 1] → dB → power → linear STFT → Griffin-Lim\n    S_db = (spec + 1.0) * (DB_RANGE / 2.0) - DB_RANGE\n    S_power = librosa.db_to_power(S_db, ref=1.0)\n    S_stft = librosa.feature.inverse.mel_to_stft(S_power, sr=SR, n_fft=N_FFT, power=2.0)\n    audio = librosa.griffinlim(S_stft, n_iter=N_ITER, hop_length=HOP_LENGTH, n_fft=N_FFT)\n\n    # Normalise to -1 dBFS peak\n    peak = np.abs(audio).max()\n    if peak > 0:\n        audio = audio / peak * 10 ** (-1.0 / 20.0)\n\n    # Save WAV\n    wav_path = f\"{output_dir}/sample_{i}.wav\"\n    sf.write(wav_path, audio, SR)\n    print(f\"Sample {i}: {len(audio)} samples ({len(audio)/SR:.2f}s) → {wav_path}\")\n\n    # Inline audio player\n    ipd.display(ipd.Audio(audio, rate=SR))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":"## 9. Save Results\n\nKaggle persists everything in `/kaggle/working/` as notebook output.\nClick **Save Version** (top right) → the training-runs zip will be available under **Output**."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import shutil\n\nshutil.make_archive(\"/kaggle/working/training-runs\", \"zip\", \".\", \"training-runs\")\nshutil.make_archive(\"/kaggle/working/reconstructed_audio\", \"zip\", \".\", \"reconstructed_audio\")\nprint(\"Created /kaggle/working/training-runs.zip\")\nprint(\"Created /kaggle/working/reconstructed_audio.zip\")\nprint(\"These will be saved as notebook output when you click Save Version.\")"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}