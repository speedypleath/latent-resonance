{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StyleGAN3 Training — Latent Resonance Spectrograms (Kaggle)\n",
    "\n",
    "Train StyleGAN3 on 512x512 grayscale spectrogram images.\n",
    "\n",
    "**Setup:** In the Kaggle sidebar, go to **Settings → Accelerator → GPU T4 x2**.\n",
    "\n",
    "**Dataset:** Upload your `spectrograms.zip` as a [Kaggle Dataset](https://www.kaggle.com/datasets),\n",
    "then add it to this notebook via **Add data** in the sidebar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb  2 06:35:08 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   36C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   39C    P8             14W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "PyTorch 2.8.0+cu126, CUDA 12.6, GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!pip install -q ninja\n",
    "\n",
    "import torch\n",
    "assert torch.cuda.is_available(), \"No GPU — enable it in Settings → Accelerator → GPU T4 x2\"\n",
    "print(f\"PyTorch {torch.__version__}, CUDA {torch.version.cuda}, GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone StyleGAN3 & Apply Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'stylegan3'...\n",
      "remote: Enumerating objects: 212, done.\u001b[K\n",
      "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
      "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
      "remote: Total 212 (delta 99), reused 90 (delta 90), pack-reused 49 (from 1)\u001b[K\n",
      "Receiving objects: 100% (212/212), 4.16 MiB | 22.91 MiB/s, done.\n",
      "Resolving deltas: 100% (108/108), done.\n",
      "Patched stylegan3/torch_utils/misc.py: InfiniteSampler fix\n",
      "Patched stylegan3/train.py: Adam betas fix\n",
      "Custom CUDA ops compiled for sm_75 — using fused kernels\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "# Fresh clone\n",
    "if os.path.exists(\"stylegan3\"):\n",
    "    shutil.rmtree(\"stylegan3\")\n",
    "!git clone https://github.com/NVlabs/stylegan3.git\n",
    "sys.path.insert(0, \"stylegan3\")\n",
    "\n",
    "# ── Patch 1: Fix InfiniteSampler for PyTorch ≥2.4 ────────────────────────\n",
    "misc_path = pathlib.Path(\"stylegan3/torch_utils/misc.py\")\n",
    "src = misc_path.read_text()\n",
    "src = src.replace(\"super().__init__(dataset)\", \"super().__init__()\")\n",
    "misc_path.write_text(src)\n",
    "print(f\"Patched {misc_path}: InfiniteSampler fix\")\n",
    "\n",
    "# ── Patch 2: Fix Adam betas int → float for PyTorch ≥2.9 ────────────────\n",
    "train_path = pathlib.Path(\"stylegan3/train.py\")\n",
    "src = train_path.read_text()\n",
    "src = src.replace(\"betas=[0,0.99]\", \"betas=[0.0,0.99]\")\n",
    "train_path.write_text(src)\n",
    "print(f\"Patched {train_path}: Adam betas fix\")\n",
    "\n",
    "# ── Patch 3: Try CUDA ops compilation ────────────────────────────────────\n",
    "# Detect GPU compute capability automatically\n",
    "cc_major, cc_minor = torch.cuda.get_device_capability(0)\n",
    "arch = f\"{cc_major}.{cc_minor}\"\n",
    "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = arch\n",
    "os.environ[\"TORCH_EXTENSIONS_DIR\"] = \"/tmp/torch_extensions\"\n",
    "if os.path.exists(\"/tmp/torch_extensions\"):\n",
    "    shutil.rmtree(\"/tmp/torch_extensions\")\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"python\", \"-c\",\n",
    "     \"import sys; sys.path.insert(0,'stylegan3'); \"\n",
    "     \"from torch_utils.ops import bias_act; \"\n",
    "     \"assert bias_act._init(), 'init failed'\"],\n",
    "    capture_output=True, text=True, timeout=180,\n",
    ")\n",
    "\n",
    "CUDA_OPS_OK = result.returncode == 0\n",
    "if CUDA_OPS_OK:\n",
    "    print(f\"Custom CUDA ops compiled for sm_{cc_major}{cc_minor} — using fused kernels\")\n",
    "else:\n",
    "    print(f\"CUDA ops compilation failed (arch {arch}), using native PyTorch fallback\")\n",
    "    print(f\"  Error: ...{result.stderr[-300:]}\")\n",
    "    ops_dir = pathlib.Path(\"stylegan3/torch_utils/ops\")\n",
    "    for name in [\"bias_act.py\", \"upfirdn2d.py\", \"filtered_lrelu.py\"]:\n",
    "        p = ops_dir / name\n",
    "        s = p.read_text()\n",
    "        s = s.replace(\"def _init():\", \"def _init():\\n    return False\")\n",
    "        p.write_text(s)\n",
    "        print(f\"  Patched {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset\n",
    "\n",
    "Kaggle datasets are mounted at `/kaggle/input/<dataset-name>/`.\n",
    "\n",
    "Set `KAGGLE_DATASET` to match your dataset name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 489 PNG files → copied to /kaggle/working/spectrograms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "KAGGLE_DATASET = \"spectrograms\"  # <-- your Kaggle dataset name\n",
    "\n",
    "input_dir = f\"/kaggle/input/{KAGGLE_DATASET}\"\n",
    "\n",
    "# Find PNGs (may be in root or a subfolder)\n",
    "pngs = glob.glob(f\"{input_dir}/**/*.png\", recursive=True)\n",
    "\n",
    "# Copy to a writable working directory (Kaggle input is read-only)\n",
    "DATASET_PATH = \"/kaggle/working/spectrograms\"\n",
    "os.makedirs(DATASET_PATH, exist_ok=True)\n",
    "for p in pngs:\n",
    "    shutil.copy(p, DATASET_PATH)\n",
    "\n",
    "print(f\"Found {len(pngs)} PNG files → copied to {DATASET_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/489 [00:00<?, ?it/s]/kaggle/working/stylegan3/dataset_tool.py:441: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  img = PIL.Image.fromarray(img, { 1: 'L', 3: 'RGB' }[channels])\n",
      "100%|████████████████████████████████████████| 489/489 [00:04<00:00, 115.46it/s]\n"
     ]
    }
   ],
   "source": [
    "!python stylegan3/dataset_tool.py \\\n",
    "    --source={DATASET_PATH} \\\n",
    "    --dest=./spectrograms.zip \\\n",
    "    --resolution=512x512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "GPUS = 2             # Kaggle T4 x2 — use both, halves per-GPU memory\n",
    "GAMMA = 2.0\n",
    "SNAP = 10\n",
    "KIMG = 5000\n",
    "CFG = \"stylegan3-t\"\n",
    "MIRROR = False\n",
    "METRICS = \"none\"\n",
    "RESUME = \"\"\n",
    "\n",
    "if CUDA_OPS_OK:\n",
    "    BATCH_SIZE = 16\n",
    "    CBASE = 32768\n",
    "    CMAX = 512\n",
    "    MBSTD = 4\n",
    "    print(f\"CUDA ops available — batch={BATCH_SIZE}, full model capacity\")\n",
    "else:\n",
    "    BATCH_SIZE = 4       # 2 per GPU\n",
    "    CBASE = 8192\n",
    "    CMAX = 128\n",
    "    MBSTD = 2\n",
    "    print(f\"Native fallback — batch={BATCH_SIZE} ({BATCH_SIZE // GPUS} per GPU), cbase={CBASE}, cmax={CMAX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA ops available — batch=8, full model capacity\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "GPUS = 1\n",
    "GAMMA = 2.0\n",
    "SNAP = 10\n",
    "KIMG = 500\n",
    "CFG = \"stylegan3-t\"\n",
    "MIRROR = False\n",
    "METRICS = \"none\"\n",
    "RESUME = \"/kaggle/working/training-runs/00005-stylegan3-t-spectrograms-gpus2-batch8-gamma2/network-snapshot-000200.pkl\"\n",
    "\n",
    "if CUDA_OPS_OK:\n",
    "    BATCH_SIZE = 8\n",
    "    CBASE = 8192\n",
    "    CMAX = 128\n",
    "    MBSTD = 4\n",
    "    print(f\"CUDA ops available — batch={BATCH_SIZE}, full model capacity\")\n",
    "else:\n",
    "    BATCH_SIZE = 2\n",
    "    CBASE = 8192\n",
    "    CMAX = 128\n",
    "    MBSTD = 2\n",
    "    print(f\"Native fallback — batch={BATCH_SIZE}, cbase={CBASE}, cmax={CMAX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "resume_flag = f\"--resume={RESUME}\" if RESUME else \"\"\n",
    "mirror_int = 1 if MIRROR else 0\n",
    "\n",
    "!python stylegan3/train.py \\\n",
    "    --outdir=./training-runs \\\n",
    "    --cfg={CFG} \\\n",
    "    --data=./spectrograms.zip \\\n",
    "    --gpus={GPUS} \\\n",
    "    --batch={BATCH_SIZE} \\\n",
    "    --batch-gpu={BATCH_SIZE} \\\n",
    "    --gamma={GAMMA} \\\n",
    "    --snap={SNAP} \\\n",
    "    --kimg={KIMG} \\\n",
    "    --mirror={mirror_int} \\\n",
    "    --metrics={METRICS} \\\n",
    "    --cbase={CBASE} \\\n",
    "    --cmax={CMAX} \\\n",
    "    --mbstd-group={MBSTD} \\\n",
    "    {resume_flag}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training-runs/00009-stylegan3-t-spectrograms-gpus2-batch8-gamma2/network-snapshot-000000.pkl\n",
      "Setting up PyTorch plugin \"bias_act_plugin\"... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0202 06:38:42.301000 146 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "W0202 06:38:42.301000 146 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Setting up PyTorch plugin \"filtered_lrelu_plugin\"... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0202 06:38:43.115000 146 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "W0202 06:38:43.115000 146 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "pkls = sorted(glob.glob(\"training-runs/**/*.pkl\", recursive=True))\n",
    "assert pkls, \"No snapshots found — has training completed at least one snapshot?\"\n",
    "latest_pkl = pkls[-1]\n",
    "print(f\"Loading {latest_pkl}\")\n",
    "\n",
    "with open(latest_pkl, \"rb\") as f:\n",
    "    G = pickle.load(f)[\"G_ema\"].cuda().eval()\n",
    "\n",
    "z = torch.randn(4, G.z_dim, device=\"cuda\")\n",
    "with torch.no_grad():\n",
    "    imgs = G(z, None)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    img = imgs[i, 0].cpu().numpy()\n",
    "    ax.imshow(img, cmap=\"magma\", aspect=\"auto\")\n",
    "    ax.set_title(f\"Sample {i}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Generated Spectrograms\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results\n",
    "\n",
    "Kaggle persists everything in `/kaggle/working/` as notebook output.\n",
    "Click **Save Version** (top right) → the training-runs zip will be available under **Output**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(\"/kaggle/working/training-runs\", \"zip\", \".\", \"training-runs\")\n",
    "print(\"Created /kaggle/working/training-runs.zip\")\n",
    "print(\"This will be saved as notebook output when you click Save Version.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
